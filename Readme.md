# ðŸ§  Hybrid Graph-RAG Medical Assistant

> ðŸ”Ž **Live Test Coverage Dashboard:**  
> https://yash2535.github.io/hybrid-graph-rag-medical-assistant/coverage/

---

## ðŸ“Œ Overview

The **Hybrid Graph-RAG Medical Assistant** is a privacy-preserving medical question-answering system that combines:

- ðŸ§© **Structured reasoning** using a Neo4j Knowledge Graph
- ðŸ“š **Semantic retrieval** using a Qdrant Vector Database
- ðŸ§  **Local LLM inference** using Ollama
- âš ï¸ **Safety validation** for medical reliability

It integrates structured patient context with research paper embeddings to generate **safe, explainable, and auditable responses** â€” fully offline.

---

## âœ¨ Key Features

- ðŸ§  Hybrid Graph + Vector Retrieval
- ðŸ¥ Patient-specific reasoning via Neo4j
- ðŸ“„ Research-backed answers via semantic search
- âš ï¸ Drug interaction & safety checks
- ðŸ“Š Structured claims output for transparency
- ðŸ”’ Fully local execution â€” no cloud dependency
- ðŸ§ª 300+ automated tests with live coverage dashboard

---

## ðŸ—ï¸ Architecture

```
User Question
      â”‚
      â–¼
Neo4j Patient Graph â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”œâ”€â”€ Context Builder â”€â”€â”€â”€â”€â”€â”€â”€â–º Local LLM (Ollama)
Qdrant Research Papers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
              Safe + Structured Medical Response
```

---

## ðŸ§© Core Components

| Layer          | Technology       | Purpose                                      |
|----------------|------------------|----------------------------------------------|
| Graph Database | Neo4j            | Patient data, conditions, medications        |
| Vector Database| Qdrant           | Semantic research paper retrieval            |
| Embeddings     | BAAI/bge-m3      | Dense vector representation                  |
| LLM            | phi3:mini (Ollama) | Local answer generation                    |
| Safety Layer   | Custom logic     | Drug interaction & red-flag checks           |
| Testing        | Pytest           | 300+ automated tests                         |

---

## ðŸ“ Project Structure

```
hybrid-graph-rag-medical-assistant/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ fetchers/
â”‚   â”‚   â””â”€â”€ pubmed_fetcher.py
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â””â”€â”€ pubmed_ingest.py
â”‚   â”œâ”€â”€ knowledge_graph/
â”‚   â”‚   â”œâ”€â”€ autopilot.py
â”‚   â”‚   â”œâ”€â”€ patient_graph_reader.py
â”‚   â”‚   â”œâ”€â”€ setup_neo4j.py
â”‚   â”‚   â””â”€â”€ wearables_graph.py
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ ollama_client.py
â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â”œâ”€â”€ chunker.py
â”‚   â”‚   â”œâ”€â”€ embedding.py
â”‚   â”‚   â””â”€â”€ entity_extractor.py
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ claim_extractor.py
â”‚   â”‚   â”œâ”€â”€ fact_checker.py
â”‚   â”‚   â”œâ”€â”€ graph_rag_pipeline.py
â”‚   â”‚   â”œâ”€â”€ prompt_builder.py
â”‚   â”‚   â””â”€â”€ qdrant_search.py
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ api.py
â”‚   â”œâ”€â”€ schema/
â”‚   â”‚   â””â”€â”€ schema_builder.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ logger.py
â”‚   â”œâ”€â”€ vector_store/
â”‚   â”‚   â”œâ”€â”€ paper_search.py
â”‚   â”‚   â””â”€â”€ qdrant_store.py
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ models.py
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ htmlcov/
â”‚   â””â”€â”€ templates/
â”œâ”€â”€ tests/
â”œâ”€â”€ app.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ pytest.ini
â””â”€â”€ README.md
```

---

## âš™ï¸ Prerequisites

### 1. Python 3.9+

### 2. Neo4j
- Neo4j Desktop or AuraDB
- Default connection: `bolt://localhost:7687`

### 3. Qdrant (via Docker)

```bash
docker run -d -p 6333:6333 qdrant/qdrant
```

Verify at: `http://localhost:6333`

### 4. Ollama (Local LLM)

Install from: https://ollama.com

```bash
ollama pull phi3:mini
ollama list   # verify installation
```

---

## ðŸ” Environment Variables

Create a `.env` file in the project root:

```env
# Neo4j
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_password

# Qdrant
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Ollama
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=phi3:mini
```

---

## ðŸš€ Setup

### 1. Create a Virtual Environment

```bash
python -m venv .venv
```

Activate:

```bash
# Windows
.venv\Scripts\activate

# Linux / macOS
source .venv/bin/activate
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Confirm All Services Are Running

| Service | Check                                          |
|---------|------------------------------------------------|
| Neo4j   | Running on `bolt://localhost:7687`             |
| Qdrant  | http://localhost:6333                          |
| Ollama  | Run `ollama serve` if not already active       |

---

## â–¶ï¸ Running the Pipeline

```bash
python -m app.rag.graph_rag_pipeline
```

The pipeline executes the following steps:

1. Update patient graph
2. Retrieve patient profile
3. Retrieve wearable summaries
4. Retrieve relevant research papers
5. Run drug interaction checks
6. Generate a safe, structured answer via LLM

---

## ðŸ§ª Testing

Run the test suite:

```bash
pytest tests/test_suite.py -v
```

Generate a local HTML coverage report:

```bash
pytest tests/test_suite.py --cov=app --cov-report=html:docs/coverage
```

ðŸ“Š Live coverage dashboard:  
https://yash2535.github.io/hybrid-graph-rag-medical-assistant/coverage/

---

## ðŸ“Œ Sample Output

```
===== FINAL ANSWER =====
Personalized medical guidance based on patient profile and research context.

===== STRUCTURED CLAIMS =====
- Risk assessment
- Monitoring advice
- Emergency warning signs
```

---

## ðŸš¨ System Requirements

| Model      | Minimum RAM |
|------------|-------------|
| phi3:mini  | ~3 GB       |
| llama3     | > 4.6 GB    |

> For systems with 8 GB RAM, **phi3:mini** is recommended.

---

## ðŸ”® Future Improvements

- GPU acceleration
- Sparse + dense hybrid retrieval
- Clinical citation linking (PMID)
- Web UI (React / Streamlit)
- FHIR-compatible patient records
- CI/CD automated coverage deployment

---

## ðŸ“œ Disclaimer

This system is intended for **educational and research purposes only**.  
It does not replace professional medical advice, diagnosis, or treatment.