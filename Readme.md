# ğŸ§  Hybrid Graph-RAG Medical Assistant

**Neo4j + Qdrant + Local LLM (Ollama)**

This project implements a **Hybrid Graph-RAG (Retrieval-Augmented Generation)** pipeline for medical question answering.  
It combines **structured patient data (Neo4j Knowledge Graph)** with **unstructured medical research papers (Qdrant Vector DB)** and generates **safe, explainable answers using a local LLM via Ollama**.

---

## âœ¨ Key Features

- ğŸ§© **Knowledge Graph Reasoning** using Neo4j (patient profile, conditions, medications)
- ğŸ“š **Semantic Paper Retrieval** using Qdrant + Transformer embeddings
- ğŸ§  **Local LLM Inference** using Ollama (no cloud dependency)
- âš ï¸ **Medical Safety Checks** (drug interactions, red-flag symptoms)
- ğŸ§¾ **Structured Claims Output** for explainability and auditing
- ğŸ”’ Fully **offline & privacy-preserving**

---

## ğŸ—ï¸ High-Level Architecture

```

User Question
â”‚
â–¼
Neo4j Patient Graph â”€â”€â”€â”€â”€â”€â”
â”œâ”€â”€ Context Builder â”€â”€â–º Local LLM (Ollama)
Qdrant Research Papers â”€â”€â”€â”˜
â”‚
â–¼
Structured & Safe Medical Answer

```

---

## ğŸ“ Project Structure

```

Main_Health/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â””â”€â”€ graph_rag_pipeline.py   # Main pipeline entry
â”‚   â”œâ”€â”€ vector_store/
â”‚   â”‚   â”œâ”€â”€ qdrant_store.py
â”‚   â”‚   â””â”€â”€ paper_search.py
â”‚   â”œâ”€â”€ graph/
â”‚   â”‚   â””â”€â”€ patient_graph.py
â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â””â”€â”€ embedding.py
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ ollama_client.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .env

````

---

## âš™ï¸ Prerequisites

Make sure the following are installed:

### 1ï¸âƒ£ Python
```bash
Python 3.9+
````

### 2ï¸âƒ£ Neo4j

* Neo4j Desktop **or** Neo4j AuraDB
* Database running and accessible

### 3ï¸âƒ£ Qdrant

Run Qdrant locally using Docker:

```bash
docker run -d -p 6333:6333 qdrant/qdrant
```

Verify:

```bash
http://localhost:6333
```

### 4ï¸âƒ£ Ollama (Local LLM)

Install Ollama from:
ğŸ‘‰ [https://ollama.com](https://ollama.com)

Pull a lightweight model (recommended):

```bash
ollama pull phi3:mini
```

Verify:

```bash
ollama list
```

---

## ğŸ” Environment Variables (`.env`)

Create a `.env` file in the project root:

```env
# Neo4j
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_password

# Qdrant
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Ollama
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=phi3:mini
```

---

## ğŸ§ª Setup Instructions

### 1ï¸âƒ£ Create Virtual Environment

```bash
python -m venv .venv
```

Activate it:

**Windows**

```bash
.venv\Scripts\activate
```

**Linux / macOS**

```bash
source .venv/bin/activate
```

---

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 3ï¸âƒ£ Ensure Services Are Running

| Service | Command / Check                    |
| ------- | ---------------------------------- |
| Neo4j   | Running on `bolt://localhost:7687` |
| Qdrant  | `http://localhost:6333`            |
| Ollama  | `ollama serve`                     |

---

## â–¶ï¸ How to Run the Entire Project

From the project root:

```bash
python -m app.rag.graph_rag_pipeline
```

This will:

1. Update patient graph from the question
2. Fetch patient profile from Neo4j
3. Fetch wearable summaries
4. Retrieve medical papers from Qdrant
5. Perform drug interaction checks
6. Generate a **safe, explainable answer using Ollama**

---

## ğŸ“Œ Sample Output

```
===== FINAL ANSWER =====
<Key medical guidance>

===== STRUCTURED CLAIMS =====
- Risk assessment
- Monitoring advice
- Emergency warning signs
```

---

## ğŸ§  Models Used

| Component  | Model                        |
| ---------- | ---------------------------- |
| Embeddings | BAAI/bge-m3                  |
| LLM        | phi3:mini (local via Ollama) |
| Vector DB  | Qdrant                       |
| Graph DB   | Neo4j                        |

---

## ğŸš¨ Notes on System Requirements

* `llama3:latest` requires **> 4.6 GB RAM**
* For 8 GB systems, **phi3:mini** is recommended
* Fully local execution (no GPU required)

---

## ğŸ”® Future Enhancements

* GPU-accelerated inference
* Hybrid dense + sparse retrieval
* Clinical citation linking (PMID)
* Web UI (Streamlit / React)
* FHIR-compliant medical records

---

## ğŸ“œ Disclaimer

This system is for **educational and research purposes only**.
It does **not** replace professional medical diagnosis or treatment.

---




